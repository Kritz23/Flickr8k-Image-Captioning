Image captioning combines computer vision and natural language processing to generate human-like descriptions for images. I used Resnet50 and LSTM approach for this task - A residual learning model leveraging the pre-trained ResNet50 CNN for improved feature extraction, followed by an LSTM network for generating captions.

The image captioning model consists of an encoder-decoder architecture:
1. Encoder: ResNet50 - Extracts visual features from the input images.
○ The ResNet50 model processes the image through multiple layers, capturing various levels of detail and patterns. It converts the image into a high-dimensional feature vector that encapsulates important visual information.
2. Decoder: LSTM - Generates a sequence of words by taking input from the encoder (image features) and captions embeddings.
○ The initial input to the LSTM is the feature vector from the ResNet50 which is then stacked onto the caption embeddings.
○ At each step, the LSTM takes the previous word and its internal state to predict the next word in the sequence.
○ This process continues until the entire caption is generated.

Two experiments were conducted using this approach of modeling purposely by tuning the steps involved in producing Numericalized captions to explore the effects of caption preprocessing.
Experiment 1 - Using textual captions as it is and applying data preprocessing steps given in section 3 without cleaning the captions.
Experiment 2 - Captions were cleaned before processing by removing stopwords and punctuations that usually add a lot of noise to text data (Should I remove stopwords when feeding sentence to RNN, 2023); to understand the impact on model learning. 

<img width="687" alt="Screenshot 2024-06-04 at 3 20 15 PM" src="https://github.com/Kritz23/Flickr8k-Image-Captioning/assets/43573202/d567b0b1-8ea5-4af5-a89d-0b2a6c9ab8a1">
The captions generated by the model trained in experiment 2 did not contain stopwords and so lacked linguistic context but it definitely gave a general idea/description of the image and its objects. 
Fig 13 shows (right) that no incorrect words were generated for the color of the shirt, so the model is quite robust.
